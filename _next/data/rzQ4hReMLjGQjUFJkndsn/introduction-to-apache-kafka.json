{"pageProps":{"article":{"blog_id":"cfb72d72-4754-48f1-b815-52edb986d17b","title":"Introduction to Apache Kafka","short_description":"In this lab we will be covering about what is the apache kafka. How it works and little bit cover about the installation and the basic workflow demonstration","description":"<div id=\"content-0\"><h1 class=\"ql-align-justify\"><strong>Introduction</strong></h1><p class=\"ql-align-justify\">Apache Kafka is an event streaming platform that helps in moving and storing large amounts of data in real-time. It is like a central hub where different sources of data can send their events, and these events can be consumed by various applications or systems.</p><p class=\"ql-align-justify\">Think of it as a postal service. Imagine you have different people sending letters from different locations, and you want all these letters to be collected in one place so that anyone who needs them can access them. Apache Kafka acts as that central place where all the letters (events) are collected and stored. It ensures that the events are delivered reliably and can be accessed by multiple applications or systems.</p><p class=\"ql-align-justify\">Kafka is highly scalable, meaning it can handle a large volume of data and process it quickly. It is also fault-tolerant, which means it can recover from failures and ensure that no data is lost. Additionally, Kafka is open source, so it can be used for free and customized according to specific needs.</p></div><div id=\"content-1\"><h1 class=\"ql-align-justify\"><strong>Architecture</strong></h1></div><div id=\"content-2\"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_kafka_architecture.png?alt=media&token=4f637bf3-00ec-4dc6-9fb9-9f5602e9d18a'/></div><div id=\"content-3\"><p class=\"ql-align-justify\">The architecture of Apache Kafka consists of several key components that work together to enable the efficient streaming and processing of data. Here is a simplified explanation of the architecture and its components:</p><ol><li class=\"ql-align-justify\">Producers: Producers are the sources of data in Kafka. They send events or messages to Kafka topics. Topics are like categories or channels where events are organized and stored.</li><li class=\"ql-align-justify\">Topics: Topics are the central entities in Kafka. They represent a specific category or stream of events. Producers send events to specific topics, and consumers subscribe to topics to receive those events.</li><li class=\"ql-align-justify\">Brokers: Brokers are the servers in the Kafka cluster. They receive events from producers, store them, and distribute them to consumers. Each broker can handle a specific amount of data and provides fault tolerance by replicating data across multiple brokers.</li><li class=\"ql-align-justify\">Consumers: Consumers are the applications or systems that subscribe to topics and receive events from Kafka. They process the events according to their specific requirements.</li><li class=\"ql-align-justify\">Partitions: Topics are divided into partitions, which are individual ordered sequences of events. Each partition is stored on a specific broker. Partitioning allows for parallel processing and scalability.</li><li class=\"ql-align-justify\">Consumer Groups: Consumer groups are a way to scale the consumption of events. Multiple consumers can be part of a consumer group, and each consumer within the group will receive a subset of the events from the topic partitions.</li><li class=\"ql-align-justify\">ZooKeeper (deprecated in newer versions): ZooKeeper is a distributed coordination service that was used in older versions of Kafka for managing the cluster and maintaining metadata. However, in newer versions, Kafka Raft (KRaft) is used to eliminate the dependency on ZooKeeper.</li><li class=\"ql-align-justify\">Kafka Connect: Kafka Connect is a framework for importing and exporting data to and from Kafka. It allows seamless integration with external systems and data sources.</li><li class=\"ql-align-justify\">Kafka Streams: Kafka Streams is a library that enables stream processing of data within Kafka. It allows developers to build real-time applications and perform transformations, aggregations, and analytics on the data.</li></ol></div><div id=\"content-4\"><h1 class=\"ql-align-justify\"><strong>Core Component</strong></h1><p class=\"ql-align-justify\">Here's a breakdown of the core components of Kafka:</p><p class=\"ql-align-justify\">1. Brokers: These are dedicated servers that receive, store, process, and distribute events. They are like the central hub for all the events.</p><p class=\"ql-align-justify\">2. Topics: These are containers or databases of events. Each topic stores specific types of events, such as logs, transactions, or metrics.</p><p class=\"ql-align-justify\">3. Partitions: Topics are divided into different partitions, which are like smaller sections within a topic. This helps with scalability and performance.</p><p class=\"ql-align-justify\">4. Replications: Partitions are duplicated and stored in different brokers. This ensures fault tolerance and allows for parallel processing of events.</p><p class=\"ql-align-justify\">5. Producers: These are client applications that publish events into topics. They can associate events with a key to ensure they go to the same partition. 6. Consumers: These are client applications that subscribe to topics and read events from them. They can read events as they occur or go back and read events from the beginning. To build an event streaming pipeline, we create topics, publish events using producers, and consume events using consumers. We can also use the Kafka command-line interface (CLI) to manage topics, producers, and consumers. Kafka provides a powerful and scalable solution for processing and analyzing streams of events.</p></div><div id=\"content-5\"><h1 class=\"ql-align-justify\"><strong>Installation</strong></h1><p class=\"ql-align-justify\">In this lab we wil going to test the apache kafka using the docker compose. Here is the file of the docker compose, so you can test it immediately without thinking how to installed it.</p></div><div id=\"content-6\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">version: '3.8'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:latest\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: confluentinc/cp-kafka:latest\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1</code></pre></div><div id=\"content-7\"><p>We can now running this docker compose by type in the terminal : docker-compose up -d. and then wait for all the setup to be completed. Once we completed we can now access the kafka container using this command:</p></div><div id=\"content-8\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">docker exec -it &lt;kafka-container-id&gt; bash</code></pre></div><div id=\"content-9\"><p>Now we are inside of the container. Now we can create a topic inside this apache kafka. we simply create a topic named test. Here is the command in the terminal:</p></div><div id=\"content-10\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">kafka-topics --create --topic test --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1</code></pre></div><div id=\"content-11\"><p>Once we create a topic now we can prepare other 2 terminal for the apache kafka demonstration. where the one terminal is used fot the producer and the other one is going to be a consumer. Lets begin with the first termnal and type:</p></div><div id=\"content-12\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">kafka-console-producer --topic test --bootstrap-server localhost:9092</code></pre></div><div id=\"content-14\"><p>Now our producer topic is available in this terminal. we can type a few messages and hit&nbsp;<code>Enter</code>&nbsp;after each message. In the second terminal we are going to make the consumer. here is the command:</p></div><div id=\"content-15\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">docker exec -it &lt;kafka-container-id&gt; kafka-console-consumer --topic test --bootstrap-server localhost:9092 --from-beginning</code></pre></div><div id=\"content-20\"><p>Now we should see the messages we produced in the previous step. Here is the output of what he have been doing so far</p></div><div id=\"content-21\"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_1.png?alt=media&token=09d39473-77b7-46a3-ac9d-2dd44b252e1a'/></div><div id=\"content-22\"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_2.png?alt=media&token=b3885d54-45ce-41a7-8b43-3eccc3c52241'/></div><div id=\"content-23\"><h1>Conclusion</h1><p>We've successfully installed Apache Kafka using Docker and tested it by producing and consuming messages. This setup can be expanded with additional configurations and services as needed for our use case.</p></div>","timestamp":"Friday, September 20, 2024 at 11:35:12 AM GMT+8","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_kafka.png?alt=media&token=50fe3034-c769-4d0b-b54c-5c0ad0bbd0e5","image_alt":"Apache Kafka","slug":"introduction-to-apache-kafka","index":"6b86b273ff34f","tags":["System Design","Message Broker"]},"recommendedPosts":[{"blog_id":"36855ea7-b37b-4b4c-91f1-27d90b9bde59","title":"Understanding Database Partitioning vs Sharding: Concepts, Benefits, and Challenges","short_description":"When dealing with large volumes of data, efficient database management becomes essential. Two widely used techniques to improve performance and scalability are database partitioning and database sharding. Although often confused, these approaches differ fundamentally in architecture, complexity, and suitable use cases. This article explores these differences in detail, helping you decide which fits your application best.","timestamp":"2025-05-17 09:42:15","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1747474398774_partition%20vs%20sharding%20db.png","image_alt":"partition vs sharding Database","slug":"Understanding-Database-Partitioning-vs-Sharding-Concepts-Benefits-and-Challenges","index":"d4735e3a265e1","tags":["Database","Database Architecture","Software Architecture","System Design"]},{"blog_id":"675f800c-08cb-459f-aa7d-44cdc9c9c169","title":"System Design Simplified: The Trade-Off Triangle You Must Master","short_description":"Behind every well-architected system is a set of tough decisions. The CAP Theorem simplifies those decisions by showing you what you must give up to keep your system fast, correct, and resilient. Learn how to apply this in real-world architecture.","timestamp":"2025-05-13 01:58:48","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1747100859417_CAP%20BG.jpg","image_alt":"CAP background","slug":"System-Design-Simplified-The-Trade-Off-Triangle-You-Must-Master","index":"d4735e3a265e1","tags":["System Design","CAP Theorem","Distributed Systems"]},{"blog_id":"5c2626c4-8a10-47f1-8c5c-b3ac2d84b69a","title":"Why Domain-Driven Design (DDD) Matters: From Chaos to Clarity in Complex Systems","short_description":"Domain-Driven Design (DDD) is a powerful approach to software development that places the business domain—not the technology—at the center of your design decisions. First introduced by Eric Evans, DDD is essential for developers and architects who want to build systems that reflect real-world complexity and change.","timestamp":"2025-05-12 04:23:04","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1747022797381_Domain-Driven-Design_cover.jpg","image_alt":"DDD Cover","slug":"Why-Domain-Driven-Design-DDD-Matters-From-Chaos-to-Clarity-in-Complex-Systems","index":"d4735e3a265e1","tags":["DDD","Software Architecture","System Design"]}]},"__N_SSG":true}