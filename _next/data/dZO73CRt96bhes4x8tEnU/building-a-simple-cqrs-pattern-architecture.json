{"pageProps":{"article":{"blog_id":"0af995c6-3bd5-405a-ad71-6ebeaa675d38","title":"Building a Simple CQRS Pattern Architecture","short_description":"In this lab we will implement simple CQRS architecture pattern using apache kafka as a message broker, elastic search as a search service and mysql database as a command service.","description":"<p><span style=\"color: rgb(255, 255, 255);\">The&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">Command Query Responsibility Segregation (CQRS)</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;pattern is an architectural pattern used to separate the&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">write</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;(commands) and&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">read</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;(queries) sides of an application. This separation ensures scalability, performance optimization, and flexibility, especially for systems with complex business logic.</span></p><p><span style=\"color: rgb(255, 255, 255);\">This article explains how to design a&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">simple CQRS pattern</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;and implement it in a practical example.</span></p><p><br></p><p><span style=\"color: rgb(255, 255, 255);\"><img src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcqrs.png?alt=media&amp;token=6eab7b0b-37d8-49f2-9137-27dadd766c96\" alt=\"CQRS Basic Pattern\" width=\"720px\"></span></p><p><br></p><h2><strong style=\"color: rgb(255, 255, 255);\">Why Do We Need CQRS?</strong></h2><p><span style=\"color: rgb(255, 255, 255);\">CQRS is designed to address challenges in systems where the&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">read</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;and&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">write</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;operations have distinct requirements. It is particularly helpful in large, complex applications with high performance, scalability, and maintainability needs. Here’s a breakdown of its importance:</span></p><p><span style=\"color: rgb(255, 255, 255);\">Here’s a detailed explanation of&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">why we need CQRS (Command Query Responsibility Segregation)</strong><span style=\"color: rgb(255, 255, 255);\">:</span></p><h3><strong style=\"color: rgb(255, 255, 255);\">Separation of Concerns</strong></h3><p><span style=\"color: rgb(255, 255, 255);\">In traditional CRUD-based architectures, the same model is often used for both reading and writing data. This can lead to problems such as:</span></p><p><span style=\"color: rgb(255, 255, 255);\">1.Bloated models trying to handle both reads and writes.</span></p><p><span style=\"color: rgb(255, 255, 255);\">2.Tight coupling between read and write logic, making it harder to change one without affecting the other.</span></p><h3><strong style=\"color: rgb(255, 255, 255);\">Optimization of Reads and Writes</strong></h3><p><span style=\"color: rgb(255, 255, 255);\">In many applications, the requirements for&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">reading data</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;differ significantly from&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">writing data</strong><span style=\"color: rgb(255, 255, 255);\">:</span></p><p><strong style=\"color: rgb(255, 255, 255);\">1.Writes</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;may require strict validation, transactional consistency, and complex domain logic.</span></p><p><strong style=\"color: rgb(255, 255, 255);\">2.Reads</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;often focus on speed, scalability, and simplicity, potentially requiring optimized or denormalized views of data.</span></p><h2><strong style=\"color: rgb(255, 255, 255);\">Pre-requisites for this lab:</strong></h2><p><span style=\"color: rgb(255, 255, 255);\">In this article we will build simple cqrs architecture with apache kafka, elastic search, and the backend service (Express.js).</span></p><p><span style=\"color: rgb(255, 255, 255);\">&nbsp;VM 1: Runs&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">Apache Kafka</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;for handling messaging and event distribution.</span></p><p><span style=\"color: rgb(255, 255, 255);\">&nbsp;VM 2: Runs&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">Elasticsearch</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;for read-optimized data storage and retrieval.</span></p><p><strong style=\"color: rgb(255, 255, 255);\">Express.js Services</strong><span style=\"color: rgb(255, 255, 255);\">:</span></p><p><span style=\"color: rgb(255, 255, 255);\">&nbsp;A&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">Command service</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;with an endpoint for inserting data into the system.</span></p><p><span style=\"color: rgb(255, 255, 255);\">&nbsp;A&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">Query service</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;to retrieve data from Elasticsearch.</span></p><p><span style=\"color: rgb(255, 255, 255);\">&nbsp;Mysql Database that connected to Express.js service</span></p><h2><strong style=\"color: rgb(255, 255, 255);\">Planned Architecture</strong></h2><p><span style=\"color: rgb(255, 255, 255);\"><img src=\"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FCQRS.webp?alt=media&amp;token=58c5dcef-485c-482d-8c6a-459473e51f04\" alt=\"Planned CQRS Architecture for this lab\" width=\"720px\"></span></p><p><span style=\"color: rgb(255, 255, 255);\">The architecture depicted in your diagram showcases a practical implementation of the CQRS (Command Query Responsibility Segregation) pattern using separate services and data stores for handling write and read operations. At its core, this design focuses on decoupling the responsibilities of updating and retrieving data, ensuring better scalability, performance, and maintainability.</span></p><p><span style=\"color: rgb(255, 255, 255);\">The&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">Command Service</strong><span style=\"color: rgb(255, 255, 255);\">, built with Express.js, serves as the entry point for handling all write operations. Whenever a client sends a request to add or update data, the Command Service writes the data to a MySQL database. This database acts as the system's primary source of truth, ensuring the durability and consistency of all data. Once the data is successfully persisted in MySQL, the Command Service publishes an event to the&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">Message Broker</strong><span style=\"color: rgb(255, 255, 255);\">, implemented with Apache Kafka. The role of Kafka here is to act as an intermediary that reliably propagates changes across the system, enabling asynchronous communication between services.</span></p><p><span style=\"color: rgb(255, 255, 255);\">On the other side of the architecture, a consumer service listens to the events broadcasted by Kafka. Whenever a new event is received, the consumer retrieves the relevant data from MySQL, transforms it if needed, and indexes it into an&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">ElasticSearch instance</strong><span style=\"color: rgb(255, 255, 255);\">. ElasticSearch, being optimized for querying and search operations, ensures that data is structured for fast retrieval. This makes it the perfect choice for systems that need to handle complex queries or search-heavy workloads without compromising performance.</span></p><p><span style=\"color: rgb(255, 255, 255);\">The&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">Read Service</strong><span style=\"color: rgb(255, 255, 255);\">, also built with Express.js, provides an API for retrieving data from ElasticSearch. By querying ElasticSearch directly, the Read Service delivers low-latency responses to clients, even under high query loads. This design ensures that the performance of the read operations does not interfere with or degrade the performance of write operations in the Command Service. The use of ElasticSearch also enables advanced search capabilities, such as full-text search, aggregations, and filtering, which are often slow or complex to implement in traditional relational databases.</span></p><p><span style=\"color: rgb(255, 255, 255);\">This architecture embodies the essence of CQRS by segregating the responsibilities of writing and querying data into distinct paths. The Command Service and MySQL handle writes and ensure data consistency, while the Read Service and ElasticSearch are optimized for delivering fast and efficient queries. The inclusion of Kafka as a Message Broker enables asynchronous processing, allowing the system to remain responsive to client requests even when downstream systems take time to process data.</span></p><h2><strong style=\"color: rgb(255, 255, 255);\">Setting Up Apache Kafka on Ubuntu Server:</strong></h2><p><span style=\"color: rgb(255, 255, 255);\">In this guide, I’ll walk you through setting up&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">Apache Kafka</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;on an&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">Ubuntu Server</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;running on a virtual machine. While you can certainly use Docker and Docker Compose for a Kafka setup, I decided to go the manual route to test the installation process. So, if you’re ready to roll up your sleeves, let’s dive in!</span></p><p><strong style=\"color: rgb(255, 255, 255);\">Step 1: Install Java</strong></p><p><span style=\"color: rgb(255, 255, 255);\">Kafka runs on the Java Virtual Machine (JVM), so the first step is to install Java. We’ll use OpenJDK 17 for this:</span></p><div><pre><code>sudo apt update\nsudo apt install openjdk-17-jdk -y\n</code></pre></div><p><span style=\"color: rgb(255, 255, 255);\">Once installed, you can verify the version with:</span></p><div><pre><code>java -version\n</code></pre></div><h4><strong style=\"color: rgb(255, 255, 255);\">Step 2: Download Kafka</strong></h4><p><span style=\"color: rgb(255, 255, 255);\">Next, we need to download the Kafka binaries. Use the following command to grab the latest Kafka release:</span></p><div><pre><code>wget https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz\n</code></pre></div><p><span style=\"color: rgb(255, 255, 255);\">After downloading, extract the archive:</span></p><div><pre><code>tar -xvf kafka_2.13-3.6.0.tgz\n</code></pre></div><p><span style=\"color: rgb(255, 255, 255);\">Now, let’s move the extracted Kafka directory to&nbsp;/opt for easier access:</span></p><div><pre><code>sudo mv kafka_2.13-3.6.0 /opt/kafka\n</code></pre></div><p><span style=\"color: rgb(255, 255, 255);\">Finally, navigate to the Kafka directory:</span></p><div><pre><code>cd /opt/kafka\n</code></pre></div><h4><strong style=\"color: rgb(255, 255, 255);\">Step 3: Configure Kafka Server Properties</strong></h4><p><span style=\"color: rgb(255, 255, 255);\">Before we start Kafka, we need to tweak its configuration a bit. Open the&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">server.properties</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;file with a text editor:</span></p><div><pre><code>nano config/server.properties\n</code></pre></div><p><span style=\"color: rgb(255, 255, 255);\">Here are a couple of key settings to look for:</span></p><ul><li><span style=\"color: rgb(255, 255, 255);\">log.dirs: This is where Kafka will store its log files. You can set it to a directory of your choice.</span></li><li><span style=\"color: rgb(255, 255, 255);\">zookeper.connect: Ensure this points to your ZooKeeper instance. If you’re running ZooKeeper locally, the default setting should work.</span></li></ul><h4><strong style=\"color: rgb(255, 255, 255);\">Step 4: Start ZooKeeper</strong></h4><p><span style=\"color: rgb(255, 255, 255);\">Kafka relies on&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">ZooKeeper</strong><span style=\"color: rgb(255, 255, 255);\">&nbsp;to manage its metadata, so we’ll need to start ZooKeeper before starting Kafka. Use the following command to get ZooKeeper up and running:</span></p><div><pre><code>bin/zookeeper-server-start.sh config/zookeeper.properties\n</code></pre></div><p><span style=\"color: rgb(255, 255, 255);\">To run ZooKeeper as a background process (so you can keep using your terminal), use this instead:</span></p><div><pre><code>bin/zookeeper-server-start.sh config/zookeeper.properties &gt; /dev/null 2&gt;&amp;1 &amp;\n</code></pre></div><h4><strong style=\"color: rgb(255, 255, 255);\">Step 5: Start the Kafka Broker</strong></h4><p><span style=\"color: rgb(255, 255, 255);\">Now that ZooKeeper is running, it’s time to fire up Kafka. Use this command to start the Kafka broker:</span></p><div><pre><code>bin/kafka-server-start.sh config/server.properties\n</code></pre></div><p><span style=\"color: rgb(255, 255, 255);\">Or, to run Kafka in the background:</span></p><div><pre><code>bin/kafka-server-start.sh config/server.properties &gt; /dev/null 2&gt;&amp;1 &amp;\n</code></pre></div><h4><strong style=\"color: rgb(255, 255, 255);\">Step 6: Test Your Kafka Setup</strong></h4><p><span style=\"color: rgb(255, 255, 255);\">Congratulations! Your Kafka instance is now up and running. Let’s do a quick test to ensure everything works as expected.</span></p><ol><li><strong style=\"color: rgb(255, 255, 255);\">Create a Topic</strong></li><li><span style=\"color: rgb(255, 255, 255);\">Kafka organizes messages into topics. Let’s create a topic named&nbsp;test-topic:</span></li></ol><div><pre><code>bin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n</code></pre></div><ol><li><strong style=\"color: rgb(255, 255, 255);\">List Topics</strong></li><li><span style=\"color: rgb(255, 255, 255);\">To confirm that the topic was created, list all topics:</span></li></ol><div><pre><code>bin/kafka-topics.sh --list --bootstrap-server localhost:9092\n</code></pre></div><ol><li><strong style=\"color: rgb(255, 255, 255);\">Start a Producer</strong></li><li><span style=\"color: rgb(255, 255, 255);\">A producer sends messages to a Kafka topic. Start the producer for&nbsp;test-topic:</span></li></ol><div><pre><code>bin/kafka-console-producer.sh --topic test-topic --bootstrap-server localhost:9092\n</code></pre></div><ol><li><span style=\"color: rgb(255, 255, 255);\">Type a few messages in the terminal, and they’ll be sent to the topic.</span></li><li><strong style=\"color: rgb(255, 255, 255);\">Start a Consumer</strong></li><li><span style=\"color: rgb(255, 255, 255);\">A consumer reads messages from a topic. Start a consumer to read messages from&nbsp;test-topic:</span></li></ol><div><pre><code>bin/kafka-console-consumer.sh --topic test-topic --from-beginning --bootstrap-server localhost:9092\n</code></pre></div><ol><li><span style=\"color: rgb(255, 255, 255);\">You should see the messages you typed in the producer terminal appear here!</span></li></ol><p><br></p><p><br></p><h2><strong style=\"color: rgb(255, 255, 255);\">Part 2: Installing Elasticsearch on a Virtual Machine</strong></h2><h4><strong style=\"color: rgb(255, 255, 255);\">Step 1: SSH into Your Server</strong></h4><p><span style=\"color: rgb(255, 255, 255);\">Connect to your EC2 instance (or VM):</span></p><div><pre><code>ssh -i /path/to/your-key.pem ec2-user@your-ec2-public-ip\n</code></pre></div><h4><strong style=\"color: rgb(255, 255, 255);\">Step 2: Install Java</strong></h4><p><span style=\"color: rgb(255, 255, 255);\">Elasticsearch also needs Java:</span></p><div><pre><code>sudo apt update\nsudo apt install -y openjdk-11-jdk\n</code></pre></div><h4><strong style=\"color: rgb(255, 255, 255);\">Step 3: Install Elasticsearch</strong></h4><div><pre><code>wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\nsudo apt install -y apt-transport-https\necho \"deb https://artifacts.elastic.co/packages/8.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-8.x.list\n\nsudo apt update\n\nsudo apt install -y elasticsearch\n</code></pre></div><h4><strong style=\"color: rgb(255, 255, 255);\">Step 4: Enable and Start Elasticsearch</strong></h4><div><pre><code>sudo systemctl enable elasticsearch\nsudo systemctl start elasticsearch\n</code></pre></div><h4><strong style=\"color: rgb(255, 255, 255);\">Step 5: Configure Elasticsearch for External Access</strong></h4><p><span style=\"color: rgb(255, 255, 255);\">Edit the configuration:</span></p><div><pre><code>sudo nano /etc/elasticsearch/elasticsearch.yml\n</code></pre></div><p><span style=\"color: rgb(255, 255, 255);\">Set these properties:</span></p><div><pre><code>network.host: 0.0.0.0\nhttp.port: 9200\n</code></pre></div><p><span style=\"color: rgb(255, 255, 255);\">Restart Elasticsearch:</span></p><div><pre><code>sudo systemctl restart elasticsearch\n</code></pre></div><h4><strong style=\"color: rgb(255, 255, 255);\">Step 6: Verify Elasticsearch</strong></h4><p><span style=\"color: rgb(255, 255, 255);\">Run:</span></p><div><pre><code>curl -X GET http://localhost:9200\n</code></pre></div><p><span style=\"color: rgb(255, 255, 255);\">You should see a JSON response!</span></p><p><br></p><h2><strong style=\"color: rgb(255, 255, 255);\">Part:3 Explanation of the Express.js POST Route for Inserting User Data and Sending to Kafka</strong></h2><div><pre><code>import express from 'express';\nimport { Kafka } from 'kafkajs';\nimport mysql from 'mysql2/promise'; // MySQL client for Node.js\n\nconst app = express();\napp.use(express.json()); // Parse JSON request bodies\n\n// Kafka producer setup\nconst kafka = new Kafka({\n&nbsp; &nbsp; clientId: 'my-app',\n&nbsp; &nbsp; brokers: ['localhost:9092'], // Replace with your Kafka broker(s)\n});\n\nconst producer = kafka.producer();\n\n// Connect Kafka producer\nasync function connectProducer() {\n&nbsp; &nbsp; await producer.connect();\n}\n\nconnectProducer().catch(console.error);\n\n// MySQL database connection setup\nconst dbConfig = {\n&nbsp; &nbsp; host: 'localhost',\n&nbsp; &nbsp; user: 'root', // Replace with your MySQL username\n&nbsp; &nbsp; password: 'root', // Replace with your MySQL password\n&nbsp; &nbsp; database: 'user', // Replace with your database name\n};\n\nlet connection;\n\nasync function connectDatabase() {\n&nbsp; &nbsp; try {\n&nbsp; &nbsp; &nbsp; &nbsp; connection = await mysql.createConnection(dbConfig);\n&nbsp; &nbsp; &nbsp; &nbsp; console.log('Connected to MySQL database');\n&nbsp; &nbsp; } catch (error) {\n&nbsp; &nbsp; &nbsp; &nbsp; console.error('Error connecting to MySQL:', error);\n&nbsp; &nbsp; &nbsp; &nbsp; process.exit(1);\n&nbsp; &nbsp; }\n}\n\nconnectDatabase();\n\n// POST route to insert user data\napp.post('/users', async (req, res) =&gt; {\n&nbsp; &nbsp; const userData = req.body;\n\n&nbsp; &nbsp; try {\n&nbsp; &nbsp; &nbsp; &nbsp; // Insert data into MySQL database\n&nbsp; &nbsp; &nbsp; &nbsp; const { name, email, password } = userData; // Assuming user data has 'name' and 'email' fields\n&nbsp; &nbsp; &nbsp; &nbsp; const [result] = await connection.execute(\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'INSERT INTO user (username, email, password) VALUES (?, ?, ?)',\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; [name, email, password]\n&nbsp; &nbsp; &nbsp; &nbsp; );\n\n&nbsp; &nbsp; &nbsp; &nbsp; console.log('User inserted into database:', result);\n\n&nbsp; &nbsp; &nbsp; &nbsp; // Send the user data to Kafka topic 'user-topic'\n&nbsp; &nbsp; &nbsp; &nbsp; await producer.send({\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; topic: 'users-topic', // Replace with your topic\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; messages: [\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { value: JSON.stringify(userData) },\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ],\n&nbsp; &nbsp; &nbsp; &nbsp; });\n\n&nbsp; &nbsp; &nbsp; &nbsp; res.status(201).json({ message: 'User data inserted into database and sent to Kafka' });\n&nbsp; &nbsp; } catch (error) {\n&nbsp; &nbsp; &nbsp; &nbsp; console.error('Error processing request:', error);\n&nbsp; &nbsp; &nbsp; &nbsp; res.status(500).json({ message: 'Error processing request' });\n&nbsp; &nbsp; }\n});\n\n// Start Express server\napp.listen(3000, () =&gt; {\n&nbsp; &nbsp; console.log('Server running on http://localhost:3000');\n});\n</code></pre></div><p><br></p><p><span style=\"color: rgb(255, 255, 255);\">In the given code snippet, an Express.js route (/users) is created to handle&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">POST requests</strong><span style=\"color: rgb(255, 255, 255);\">. This route processes user data by first saving it into a MySQL database and then sending the same data to a Kafka topic. Below is a step-by-step explanation of how this route works:</span></p><h4><strong style=\"color: rgb(255, 255, 255);\">1.&nbsp;Endpoint Definition and Request Handling</strong></h4><div><pre><code>The app.post('/users', async (req, res) defines a route that listens for POST requests at the /users endpoint. It uses async/await to handle asynchronous operations such as database insertion and Kafka messaging. The req.body object is used to extract the data sent by the client in the request payload.const userData = req.body;\n</code></pre></div><p><span style=\"color: rgb(255, 255, 255);\">The&nbsp;userData object contains the user-provided information, typically in JSON format. For example, it might include fields like&nbsp;name. email and password.</span></p><h4><strong style=\"color: rgb(255, 255, 255);\">2.&nbsp;Inserting User Data into MySQL</strong></h4><p><span style=\"color: rgb(255, 255, 255);\">To store user data, the route uses a prepared SQL statement to prevent&nbsp;</span><strong style=\"color: rgb(255, 255, 255);\">SQL injection attacks</strong><span style=\"color: rgb(255, 255, 255);\">. The&nbsp;</span>connection.execute()<span style=\"color: rgb(255, 255, 255);\">&nbsp;function interacts with the database, where&nbsp;name, email and passwords fields are inserted into a&nbsp;user table.</span></p><div><pre><code>const [result] = await connection.execute(\n    'INSERT INTO user (username, email, password) VALUES (?, ?, ?)',\n    [name, email, password]\n);\n</code></pre></div><ul><li><strong style=\"color: rgb(255, 255, 255);\">Prepared Statement</strong><span style=\"color: rgb(255, 255, 255);\">: The&nbsp;? placeholders in the SQL query are replaced with actual values (name, email, password) safely.</span></li><li><strong style=\"color: rgb(255, 255, 255);\">Deconstructed User Data</strong><span style=\"color: rgb(255, 255, 255);\">: The&nbsp;name,&nbsp;email, and&nbsp;password fields are extracted from the&nbsp;userData object for better readability and security.</span></li><li><strong style=\"color: rgb(255, 255, 255);\">Result</strong><span style=\"color: rgb(255, 255, 255);\">: The&nbsp;connection.execite()&nbsp;method returns an array, where&nbsp;result contains metadata about the operation, such as the number of rows affected.</span></li></ul><p><span style=\"color: rgb(255, 255, 255);\">If the operation succeeds, a log is generated to confirm that the user data was inserted into the database:</span></p><div><pre><code>console.log('User inserted into database:', result);\n</code></pre></div><h4><strong style=\"color: rgb(255, 255, 255);\">3.&nbsp;Sending Data to a Kafka Topic</strong></h4><p><span style=\"color: rgb(255, 255, 255);\">After successfully storing the data in MySQL, the route sends the same data to a Kafka topic for further processing. Kafka is often used to handle large-scale distributed messaging and stream processing.</span></p><div><pre><code>await producer.send({\n    topic: 'users-topic', // Replace with your topic\n    messages: [\n        { value: JSON.stringify(userData) },\n    ],\n});\n</code></pre></div><ul><li><strong style=\"color: rgb(255, 255, 255);\">Kafka Producer</strong><span style=\"color: rgb(255, 255, 255);\">p: The producer object is an instance of Kafka's producer client, which is responsible for sending messages to Kafka topics.</span></li><li><strong style=\"color: rgb(255, 255, 255);\">Topic Name</strong><span style=\"color: rgb(255, 255, 255);\">: The&nbsp;topic field specifies the destination Kafka topic (user-topic in this case). This is where the message will be sent for further processing by Kafka consumers.</span></li><li><strong style=\"color: rgb(255, 255, 255);\">Message Payload</strong><span style=\"color: rgb(255, 255, 255);\">: The&nbsp;message array contains the data to be sent. Each message is an object with a&nbsp;value field, which holds the serialized user data (converted to JSON using&nbsp;JSON.stringfy(userData)).</span></li></ul><p><span style=\"color: rgb(255, 255, 255);\">This mechanism ensures that user data is available for other systems (e.g., analytics, logging, or notifications) in near real-time.</span></p><h4><strong style=\"color: rgb(255, 255, 255);\">4.&nbsp;Response to the Client</strong></h4><p>If both the database insertion and Kafka message-sending steps succeed, the server sends a 201 Created response to the client with a success message:</p><div><pre><code>res.status(201).json({ message: 'User data inserted into database and sent to Kafka' });\n</code></pre></div><p>The 201 status code indicates that the request was successfully processed and a new resource was created.</p><h4><strong style=\"color: rgb(255, 255, 255);\">5.&nbsp;Error Handling</strong></h4><p>The try-catch block ensures that errors during either database insertion or Kafka messaging are gracefully handled. If an error occurs, it is logged for debugging purposes, and the client receives a 500 Internal Server Error response:</p><div><pre><code>catch (error) {\n    console.error('Error processing request:', error);\n    res.status(500).json({ message: 'Error processing request' });\n}\n</code></pre></div><p><span style=\"color: rgb(255, 255, 255);\">This approach provides transparency to developers and prevents the application from crashing due to unhandled exceptions.</span></p><p><br></p><p><span style=\"color: rgb(255, 255, 255);\">After building the express.js service to insert data in mysql database and message broker. we will create the instance that listening on this producer.</span></p><p><br></p><h2><strong style=\"color: rgb(255, 255, 255);\">Part 4: Explanation of Kafka Consumer with Elasticsearch Integration</strong></h2><div><pre><code>import { Kafka } from 'kafkajs';\nimport { Client } from '@elastic/elasticsearch';\n\n// Kafka consumer setup\nconst kafka = new Kafka({\n&nbsp; &nbsp; clientId: 'express-app',\n&nbsp; &nbsp; brokers: ['192.168.128.207:9092'], // Replace with your Kafka broker(s)\n});\n\nconst consumer = kafka.consumer({ groupId: 'user-group' });\n\n// Elasticsearch client setup\nconst esClient = new Client({\n&nbsp; &nbsp; node: 'http://localhost:9200', // Replace with your Elasticsearch URL\n});\n\n// Kafka consumer processing\nasync function consumeMessages() {\n&nbsp; &nbsp; await consumer.connect();\n&nbsp; &nbsp; await consumer.subscribe({ topic: 'users-topic', fromBeginning: true }); // Replace with your topic\n\n&nbsp; &nbsp; await consumer.run({\n&nbsp; &nbsp; &nbsp; &nbsp; eachMessage: async ({ topic, partition, message }) =&gt; {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const userData = JSON.parse(message.value.toString());\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // Insert data into Elasticsearch\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; try {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; const response = await esClient.index({\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; index: 'users', // The Elasticsearch index to use\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; id: message.name,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; document: userData,\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; });\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; console.log('User data inserted into Elasticsearch:', response);\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } catch (error) {\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; console.error('Error inserting data into Elasticsearch:', error);\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }\n&nbsp; &nbsp; &nbsp; &nbsp; },\n&nbsp; &nbsp; });\n}\n\nconsumeMessages().catch(console.error);\n</code></pre></div><p>This code demonstrates how to integrate Kafka as a messaging system and Elasticsearch as a search and data indexing tool in a Node.js application. The overall flow involves consuming messages from a Kafka topic and then indexing the received data into Elasticsearch for further use, such as querying or searching.</p><p>To start, the script imports two essential libraries: KafkaJS and Elasticsearch client. KafkaJS is a JavaScript library used for interacting with Kafka, which is a distributed streaming platform. The Kafka client allows you to create consumers that can listen to Kafka topics and process the messages in real time. On the other hand, the Elasticsearch client facilitates communication with an Elasticsearch cluster, enabling the ability to store and index documents, which can later be queried or analyzed.</p><p>The Kafka consumer is set up by first initializing the Kafka client with a unique clientId (express-app) and specifying the Kafka brokers. These brokers are the Kafka servers where the consumer will connect. The consumer is created with a groupId, which is user-group in this case. The group ID helps manage message consumption across multiple instances of the consumer. When consumers with the same group ID listen to a Kafka topic, Kafka ensures that each partition of the topic is assigned to only one consumer in the group, effectively balancing the load.</p><p>Next, the code sets up the Elasticsearch client by specifying the address of the Elasticsearch node (http://localhost:9200). This client will be used to interact with the Elasticsearch service where the user data will be indexed. Elasticsearch is widely used for its powerful search and analytics capabilities, which can handle large volumes of data and provide fast search results.</p><p>Once both Kafka and Elasticsearch clients are set up, the consumeMessages() function is created to handle the actual logic of consuming messages from Kafka. This function first connects to the Kafka cluster and subscribes to the users-topic. By subscribing to the topic, the consumer listens for new messages that are published to that topic. The fromBeginning: true option ensures that the consumer starts processing messages from the very beginning of the topic’s log, meaning it will consume all the messages from when it first subscribes, not just new messages that arrive after it subscribes.</p><p>The function then uses consumer.run() to begin consuming messages. Each message from the Kafka topic is processed in the eachMessage callback function. Inside this function, the message's value is parsed from a buffer into a JavaScript object (since Kafka messages are typically sent as binary data). The parsed data, which represents user information in this case, is then indexed into Elasticsearch. The esClient.index() method is used to insert this data into the users index in Elasticsearch. A unique identifier for the document is generated using the message's name field. This id ensures that each document in Elasticsearch can be uniquely identified.</p><p>If the data insertion into Elasticsearch is successful, a response from Elasticsearch is logged to the console, confirming that the user data has been indexed. If an error occurs while inserting the data, the error is caught and logged.</p><p>Finally, the consumeMessages() function is invoked, and any unhandled errors are caught by console.error() to prevent the application from crashing. This ensures that the consumer will keep running and processing messages as they arrive, continuously feeding new data into Elasticsearch for indexing.</p><h2><strong style=\"color: rgb(255, 255, 255);\">Part 5: Wrapup what we have been doing</strong></h2><p>The architecture discussed in this article aligns well with the CQRS (Command Query Responsibility Segregation) pattern, which is a powerful design pattern that separates the logic of reading data (queries) from the logic of writing data (commands). By implementing Kafka and Elasticsearch in conjunction with MySQL and Express.js, we create a robust system that effectively adheres to the principles of CQRS.</p><p>In this architecture, the write operations (commands) are handled by the /users POST route in the Express.js service. When user data is received, it's inserted into the MySQL database and sent to Kafka. Kafka acts as the message bus, decoupling the data-writing process from the read operations and ensuring that data can be asynchronously processed and consumed by different systems or services.</p><p>The read operations (queries) are efficiently handled by Elasticsearch. After the data is consumed from Kafka and indexed into Elasticsearch, it becomes readily available for fast and scalable querying. Elasticsearch's ability to index and search large volumes of data makes it an excellent fit for handling query-based operations in this architecture.</p><p>By using CQRS, we ensure that the system is optimized for both reading and writing operations, enabling high scalability and responsiveness. Kafka, as the message broker, enables asynchronous communication and allows for horizontal scaling in the system. Meanwhile, Elasticsearch ensures that queries on user data are fast, efficient, and scalable.</p><p>This CQRS-based approach also helps with performance optimization, as read and write concerns are handled separately. It allows for the scaling of each part of the system independently, depending on whether there is a higher load on reading or writing. The separation of concerns promotes better maintainability, flexibility, and scalability, making this architecture ideal for modern, high-traffic applications requiring real-time data processing and analytics.</p><p>In conclusion, by integrating Kafka, Elasticsearch, and Express.js with a CQRS approach, this system architecture offers a scalable, maintainable, and highly performant solution for handling real-time data in applications where reading and writing data need to be optimized separately.</p>","timestamp":"Saturday, December 7, 2024 at 9:06:28 AM GMT+8","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fcqrs-background-title.png?alt=media&token=dd34dffa-1cc4-4b14-b4e2-f6840555b0e4","image_alt":"Image cover of CQRS Architecture","slug":"building-a-simple-cqrs-pattern-architecture","index":"6b86b273ff34f","tags":["SQL","Backend","System Design","Software Architecture","API"]},"recommendedPosts":[{"blog_id":"4400b3a0-4d34-4185-806a-f265089e8af8","title":"MySQL Migration with Connection Pooling: A Simple Guide","short_description":"Imagine standing in line at a coffee shop where each customer needs to fill out a membership form before ordering and then tears it up after getting their coffee. Sounds inefficient, right? This is exactly what happens when your application connects to a database without connection pooling.","timestamp":"2025-04-28 11:49:06","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1745838714722_connection%20pool%20Bg.png","image_alt":"Connection Pool Labs Content","slug":"MySQL-Migration-with-Connection-Pooling-A-Simple-Guide-1","index":"d4735e3a265e1","tags":["System Design","Database","Software Architecture","Cloud Computing"]},{"blog_id":"6234fef8-1547-46f7-ae10-33d577a1d168","title":"Understanding RabbitMQ: A Favorite Simple Messaging Service!","short_description":"RabbitMQ is a robust, open-source message broker that facilitates communication between applications by sending and receiving messages. Whether you're building a microservices architecture or a distributed system, RabbitMQ ensures reliable, scalable, and asynchronous messaging. In this blog, we’ll walk through its core components and concepts, from producers to consumers, and dive into some advanced features like round-robin dispatching and virtual hosts.","timestamp":"2025-03-15 19:44:13","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1742090540692_rabbitmq.webp","image_alt":"rabbit mq","slug":"Understanding-RabbitMQ-A-Favorite-Simple-Messaging-Service","index":"6b86b273ff34f","tags":["Message Broker","System Design","Software Architecture"]},{"blog_id":"86f7440f-033f-4459-b0a5-09f74d7c34ba","title":"Understanding Circuit Breakers in Software Engineering: From Traditional to Serverless","short_description":"Imagine you’re using electricity at home, and a short circuit occurs. The circuit breaker in your electrical panel cuts the power to prevent a fire. In software, the concept is similar: it’s a design pattern that protects your system from repeated failures when calling external services","timestamp":"2025-03-14 02:46:27","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1741948558177_circuit_breaker.png","image_alt":"Circuit breaker","slug":"Understanding-Circuit-Breakers-in-Software-Engineering-From-Traditional-to-Serverless","index":"6b86b273ff34f","tags":["System Design","Software Architecture"]}]},"__N_SSG":true}