{"pageProps":{"article":{"blog_id":"59ebedc0-f362-4c2b-a7ee-a5fd8db2bc29","title":"Create a DAG for Apache Airflow with Python Operator","short_description":"In this lab, you will explore the Apache Airflow web user interface (UI). You will then create a Direct Acyclic Graph (DAG) using PythonOperator and finally run it through the Airflow web UI.","description":"<div id=\"content-0\"><p class=\"ql-align-justify\">Apache Airflow is an open-source platform designed for orchestrating and managing complex workflows and data pipelines. It allows users to programmatically author, schedule, and monitor workflows through a rich web-based user interface. Airflow uses directed acyclic graphs (DAGs) to represent workflows, making it highly flexible and scalable for a wide range of use cases. With built-in support for dynamic pipeline generation, Airflow enables users to create workflows that adapt to changes in data and business logic. Additionally, its extensible architecture allows integration</p></div><div id=\"content-1\"><img src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_architecture.png?alt=media&token=2ff31468-d678-4134-b46c-4f0da4f4ff93'/></div><div id=\"content-3\"><p class=\"ql-align-justify\">Apache Airflow's architecture is designed to be modular and scalable, consisting of several key components: the Scheduler, the Executor, the Web Server, and the Metadata Database. The Scheduler handles the scheduling of tasks, ensuring they run at the correct times and in the correct order. The Executor runs the tasks, which can be distributed across multiple workers for scalability. The Web Server provides a user-friendly interface for managing and monitoring workflows, while the Metadata Database stores the state of tasks and workflows.</p><p class=\"ql-align-justify\">Workflows in Airflow are defined as Directed Acyclic Graphs (DAGs), where each node represents a task, and edges define the dependencies between tasks. This structure allows for clear visualization of the workflow and its dependencies. Tasks within a DAG can range from simple data processing steps to complex data transfer operations, and they can be scheduled to run at specific intervals or triggered by external events. Airflow supports a wide range of operators for different tasks, including BashOperator for running bash scripts, PythonOperator for executing Python code, and many more for interacting with various data systems and services. This flexibility and the ability to create dynamic, code-driven workflows make Airflow a powerful tool for orchestrating data pipelines and workflows.</p><p class=\"ql-align-justify\">In this lab we will learn how to install apache airflow and create some basic DAG to perform ETL task. First we want to make sure our apache airflow in in our local machine for demonstration. We will install using docker and docker compose for it. Lets create new folder and named it as we want. In this case im going to create folder name apache_airflow. Then lets&nbsp;create a Dockerfile inside that folder and fill the file with this code:</p><p><br></p></div><div id=\"content-4\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">FROM apache/airflow:latest\nUSER root\nRUN apt-get update && \\\n    apt-get -y install git && \\\n    apt-get clean\n\nUSER airflow\n</code></pre></div><div id=\"content-5\"><p class=\"ql-align-justify\">Overall, this Dockerfile snippet customizes the Airflow Docker image by adding Git, which can be useful for tasks such as cloning repositories or managing code directly within the Airflow environment. Lets run this docker file. If the build image is successful, we then jump to the next step where we are going to create a new file docker compose to use our image and create a volums for our data. Below is the example of how we build our compose to run apache airflow:</p></div><div id=\"content-6\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">version: '3'\nservices:\n  apacheairflow:\n    image: apacheairflow:latest\n\n    volumes:\n      - ./airflow:/opt/airflow\n\n    ports:\n      - \"8080:8080\"\n    command: airflow standalone\n</code></pre></div><div id=\"content-7\"><p class=\"ql-align-justify\">After this, we can access our apache airflow in our local machine by accessing trough <a href=\"http://localhost:8080/\" target=\"_blank\">http://localhost:8080</a>. Dont forget to build up our compose file. By running docker-compose up -d. We can type our username Admin and we wil get our password from our folder generated from /airflow/stand_alone_admin_password.txt.</p><p class=\"ql-align-justify\">Now we can create our DAG file in our /airflow folder. Create new folder name dags and fill the python file inside of it. The name is our wishes.&nbsp;Then we can create a simple task from it. But we need to import some thing like:</p></div><div id=\"content-8\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\"># Import the libraries\nfrom datetime import timedelta\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow.models import DAG\n# Operators; you need this to write tasks!\nfrom airflow.operators.python import PythonOperator\n\n# This makes scheduling easy\nfrom airflow.utils.dates import days_ago</code></pre></div><div id=\"content-9\"><p class=\"ql-align-justify\">Now we can define our function, DAG Argument, DAG Definitions, Define the task, and create a task pipeline from this file.</p><p class=\"ql-align-justify\">In this case. I just want to grab some data from the api and save it to json file.Lets create our functions first called extract here is the code for the extract data from the api random jokes:</p></div><div id=\"content-10\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">def extract():\n    response = requests.get('https://official-joke-api.appspot.com/random_joke')\n    joke = response.json()\n\n    # Define the path to the JSON file\n    file_path = '/opt/airflow/jokes/joke.json'\n    \n    try:\n        # Ensure the directory exists\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        \n        # Read existing jokes from the JSON file if it exists\n        if os.path.exists(file_path):\n            with open(file_path, 'r') as f:\n                # Try to load the existing jokes, handle if file is empty\n                content = f.read()\n                jokes = json.loads(content) if content else []\n        else:\n            jokes = []\n\n        # Append the new joke to the list\n        jokes.append(joke)\n        \n        # Write the updated list of jokes to the JSON file\n        with open(file_path, 'w') as f:\n            json.dump(jokes, f, indent=4)\n        \n        print(f\"Joke saved to {file_path}: {joke['setup']} - {joke['punchline']}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n        raise</code></pre></div><div id=\"content-11\"><p>After that we can create our DAG Arguments and DAG Definitions:</p></div><div id=\"content-12\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\"># You can override them on a per-task basis during operator initialization\ndefault_args = {\n    'owner': 'Darmawan',\n    'start_date': days_ago(0),\n    'email': ['darmawanjr88@gmail.com'],\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# Define the DAG\ndag = DAG(\n    'jokes-callable-dag',\n    default_args=default_args,\n    description='My first DAG',\n    schedule_interval=timedelta(days=1),\n)\n</code></pre></div><div id=\"content-13\"><p>Then we can define the task named execute_extract to call the extract function</p></div><div id=\"content-14\"><p>And build the the task pipeline for now we just have a single task when we have multiple task we can call our defined task like a sequence call like task_1 &gt;&gt; task_2 &gt;&gt; task_3 and so on. Now we can go to the web UI and triggering the DAG that we build.</p></div><div id=\"content-15\"><img src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_1.png?alt=media&token=1b05157a-20dc-4748-bb40-ff9bbaeb23be'/></div><div id=\"content-16\"><p>Horray we dit it! We can create a simple task from the apache airflow. We can make another complex task from our idea and our problems that we faced from our life.</p></div>","timestamp":"Friday, September 20, 2024 at 11:35:46 AM GMT+8","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_airflow_python.jpg?alt=media&token=f6c31e13-c63f-49c5-b0ad-b54ec8509560","image_alt":"Apache airflow with Python","slug":"create-a-dag-for-apache-airflow-with-python-operator","index":"6b86b273ff34f","tags":["Data","Python"]},"recommendedPosts":[{"blog_id":"19882a74-d1c2-4b31-837e-99cdc1846fcf","title":"Apache Cassandra: The NoSQL Powerhouse","short_description":"In today's world of big data, scalability and performance are crucial. Apache Cassandra, an open-source NoSQL database, is a top choice for handling large-scale, distributed data. Used by giants like Facebook, Netflix, and Twitter, Cassandra offers high availability, fault tolerance, and seamless scalability. Let’s dive into its architecture and key concepts!","timestamp":"2025-03-14 01:26:37","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1741944106846_apache_cassandra.png","image_alt":"Apache Cassandra","slug":"Apache-Cassandra-The-NoSQL-Powerhouse","index":"6b86b273ff34f","tags":["NO SQL","Data"]},{"blog_id":"6b4113f2-f30c-4e12-a34a-f5c02abbd1cb","title":"Mastering Apache Spark: An Engaging Dive into Its Architecture and Clusters","short_description":"Welcome to an in-depth exploration of Apache Spark’s architecture! Whether you’re new to Spark or looking to refresh your understanding, this interactive guide will walk you through the key concepts that power Spark’s ability to process massive datasets quickly and efficiently.","timestamp":"2024-10-06 21:37:09","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fspark.png?alt=media&token=7a343f5c-0174-4a31-99e1-4943f9e135af","image_alt":"Apache Spark","slug":"mastering-apache-spark-an-engaging-dive-into-its-architecture-and-clusters","index":"6b86b273ff34f","tags":["Data","Software Architecture","System Design","Storage"]},{"blog_id":"9f224db5-a76d-4a6f-9f7b-32bfbdf5696f","title":"MapReduce: The Magic Behind Processing Big Data in Hadoop","short_description":"Ever wondered how companies handle mountains of data efficiently? Enter MapReduce—Hadoop’s superhero when it comes to processing large datasets. Instead of one machine trying to handle everything, MapReduce breaks the work into smaller chunks and distributes it across many machines, making the process faster and more reliable.","timestamp":"2024-09-28 21:50:56","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fmap_reduce.webp?alt=media&token=9675cfce-94c9-4495-9f54-d6f651de19ff","image_alt":"Map reduce","slug":"mapreduce-the-magic-behind-processing-big-data-in-hadoop","index":"6b86b273ff34f","tags":["Data","Software Architecture","System Design","Storage"]}]},"__N_SSG":true}