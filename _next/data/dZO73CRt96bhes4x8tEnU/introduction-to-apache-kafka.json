{"pageProps":{"article":{"blog_id":"cfb72d72-4754-48f1-b815-52edb986d17b","title":"Introduction to Apache Kafka","short_description":"In this lab we will be covering about what is the apache kafka. How it works and little bit cover about the installation and the basic workflow demonstration","description":"<div id=\"content-0\"><h1 class=\"ql-align-justify\"><strong>Introduction</strong></h1><p class=\"ql-align-justify\">Apache Kafka is an event streaming platform that helps in moving and storing large amounts of data in real-time. It is like a central hub where different sources of data can send their events, and these events can be consumed by various applications or systems.</p><p class=\"ql-align-justify\">Think of it as a postal service. Imagine you have different people sending letters from different locations, and you want all these letters to be collected in one place so that anyone who needs them can access them. Apache Kafka acts as that central place where all the letters (events) are collected and stored. It ensures that the events are delivered reliably and can be accessed by multiple applications or systems.</p><p class=\"ql-align-justify\">Kafka is highly scalable, meaning it can handle a large volume of data and process it quickly. It is also fault-tolerant, which means it can recover from failures and ensure that no data is lost. Additionally, Kafka is open source, so it can be used for free and customized according to specific needs.</p></div><div id=\"content-1\"><h1 class=\"ql-align-justify\"><strong>Architecture</strong></h1></div><div id=\"content-2\"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_kafka_architecture.png?alt=media&token=4f637bf3-00ec-4dc6-9fb9-9f5602e9d18a'/></div><div id=\"content-3\"><p class=\"ql-align-justify\">The architecture of Apache Kafka consists of several key components that work together to enable the efficient streaming and processing of data. Here is a simplified explanation of the architecture and its components:</p><ol><li class=\"ql-align-justify\">Producers: Producers are the sources of data in Kafka. They send events or messages to Kafka topics. Topics are like categories or channels where events are organized and stored.</li><li class=\"ql-align-justify\">Topics: Topics are the central entities in Kafka. They represent a specific category or stream of events. Producers send events to specific topics, and consumers subscribe to topics to receive those events.</li><li class=\"ql-align-justify\">Brokers: Brokers are the servers in the Kafka cluster. They receive events from producers, store them, and distribute them to consumers. Each broker can handle a specific amount of data and provides fault tolerance by replicating data across multiple brokers.</li><li class=\"ql-align-justify\">Consumers: Consumers are the applications or systems that subscribe to topics and receive events from Kafka. They process the events according to their specific requirements.</li><li class=\"ql-align-justify\">Partitions: Topics are divided into partitions, which are individual ordered sequences of events. Each partition is stored on a specific broker. Partitioning allows for parallel processing and scalability.</li><li class=\"ql-align-justify\">Consumer Groups: Consumer groups are a way to scale the consumption of events. Multiple consumers can be part of a consumer group, and each consumer within the group will receive a subset of the events from the topic partitions.</li><li class=\"ql-align-justify\">ZooKeeper (deprecated in newer versions): ZooKeeper is a distributed coordination service that was used in older versions of Kafka for managing the cluster and maintaining metadata. However, in newer versions, Kafka Raft (KRaft) is used to eliminate the dependency on ZooKeeper.</li><li class=\"ql-align-justify\">Kafka Connect: Kafka Connect is a framework for importing and exporting data to and from Kafka. It allows seamless integration with external systems and data sources.</li><li class=\"ql-align-justify\">Kafka Streams: Kafka Streams is a library that enables stream processing of data within Kafka. It allows developers to build real-time applications and perform transformations, aggregations, and analytics on the data.</li></ol></div><div id=\"content-4\"><h1 class=\"ql-align-justify\"><strong>Core Component</strong></h1><p class=\"ql-align-justify\">Here's a breakdown of the core components of Kafka:</p><p class=\"ql-align-justify\">1. Brokers: These are dedicated servers that receive, store, process, and distribute events. They are like the central hub for all the events.</p><p class=\"ql-align-justify\">2. Topics: These are containers or databases of events. Each topic stores specific types of events, such as logs, transactions, or metrics.</p><p class=\"ql-align-justify\">3. Partitions: Topics are divided into different partitions, which are like smaller sections within a topic. This helps with scalability and performance.</p><p class=\"ql-align-justify\">4. Replications: Partitions are duplicated and stored in different brokers. This ensures fault tolerance and allows for parallel processing of events.</p><p class=\"ql-align-justify\">5. Producers: These are client applications that publish events into topics. They can associate events with a key to ensure they go to the same partition. 6. Consumers: These are client applications that subscribe to topics and read events from them. They can read events as they occur or go back and read events from the beginning. To build an event streaming pipeline, we create topics, publish events using producers, and consume events using consumers. We can also use the Kafka command-line interface (CLI) to manage topics, producers, and consumers. Kafka provides a powerful and scalable solution for processing and analyzing streams of events.</p></div><div id=\"content-5\"><h1 class=\"ql-align-justify\"><strong>Installation</strong></h1><p class=\"ql-align-justify\">In this lab we wil going to test the apache kafka using the docker compose. Here is the file of the docker compose, so you can test it immediately without thinking how to installed it.</p></div><div id=\"content-6\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">version: '3.8'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:latest\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: confluentinc/cp-kafka:latest\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1</code></pre></div><div id=\"content-7\"><p>We can now running this docker compose by type in the terminal : docker-compose up -d. and then wait for all the setup to be completed. Once we completed we can now access the kafka container using this command:</p></div><div id=\"content-8\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">docker exec -it &lt;kafka-container-id&gt; bash</code></pre></div><div id=\"content-9\"><p>Now we are inside of the container. Now we can create a topic inside this apache kafka. we simply create a topic named test. Here is the command in the terminal:</p></div><div id=\"content-10\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">kafka-topics --create --topic test --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1</code></pre></div><div id=\"content-11\"><p>Once we create a topic now we can prepare other 2 terminal for the apache kafka demonstration. where the one terminal is used fot the producer and the other one is going to be a consumer. Lets begin with the first termnal and type:</p></div><div id=\"content-12\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">kafka-console-producer --topic test --bootstrap-server localhost:9092</code></pre></div><div id=\"content-14\"><p>Now our producer topic is available in this terminal. we can type a few messages and hit&nbsp;<code>Enter</code>&nbsp;after each message. In the second terminal we are going to make the consumer. here is the command:</p></div><div id=\"content-15\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">docker exec -it &lt;kafka-container-id&gt; kafka-console-consumer --topic test --bootstrap-server localhost:9092 --from-beginning</code></pre></div><div id=\"content-20\"><p>Now we should see the messages we produced in the previous step. Here is the output of what he have been doing so far</p></div><div id=\"content-21\"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_1.png?alt=media&token=09d39473-77b7-46a3-ac9d-2dd44b252e1a'/></div><div id=\"content-22\"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_2.png?alt=media&token=b3885d54-45ce-41a7-8b43-3eccc3c52241'/></div><div id=\"content-23\"><h1>Conclusion</h1><p>We've successfully installed Apache Kafka using Docker and tested it by producing and consuming messages. This setup can be expanded with additional configurations and services as needed for our use case.</p></div>","timestamp":"Friday, September 20, 2024 at 11:35:12 AM GMT+8","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fapache_kafka.png?alt=media&token=50fe3034-c769-4d0b-b54c-5c0ad0bbd0e5","image_alt":"Apache Kafka","slug":"introduction-to-apache-kafka","index":"6b86b273ff34f","tags":["Message Broker","System Design"]},"recommendedPosts":[{"blog_id":"4400b3a0-4d34-4185-806a-f265089e8af8","title":"MySQL Migration with Connection Pooling: A Simple Guide","short_description":"Imagine standing in line at a coffee shop where each customer needs to fill out a membership form before ordering and then tears it up after getting their coffee. Sounds inefficient, right? This is exactly what happens when your application connects to a database without connection pooling.","timestamp":"2025-04-28 11:49:06","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1745838714722_connection%20pool%20Bg.png","image_alt":"Connection Pool Labs Content","slug":"MySQL-Migration-with-Connection-Pooling-A-Simple-Guide-1","index":"d4735e3a265e1","tags":["System Design","Database","Software Architecture","Cloud Computing"]},{"blog_id":"6234fef8-1547-46f7-ae10-33d577a1d168","title":"Understanding RabbitMQ: A Favorite Simple Messaging Service!","short_description":"RabbitMQ is a robust, open-source message broker that facilitates communication between applications by sending and receiving messages. Whether you're building a microservices architecture or a distributed system, RabbitMQ ensures reliable, scalable, and asynchronous messaging. In this blog, we’ll walk through its core components and concepts, from producers to consumers, and dive into some advanced features like round-robin dispatching and virtual hosts.","timestamp":"2025-03-15 19:44:13","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1742090540692_rabbitmq.webp","image_alt":"rabbit mq","slug":"Understanding-RabbitMQ-A-Favorite-Simple-Messaging-Service","index":"6b86b273ff34f","tags":["Message Broker","System Design","Software Architecture"]},{"blog_id":"86f7440f-033f-4459-b0a5-09f74d7c34ba","title":"Understanding Circuit Breakers in Software Engineering: From Traditional to Serverless","short_description":"Imagine you’re using electricity at home, and a short circuit occurs. The circuit breaker in your electrical panel cuts the power to prevent a fire. In software, the concept is similar: it’s a design pattern that protects your system from repeated failures when calling external services","timestamp":"2025-03-14 02:46:27","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1741948558177_circuit_breaker.png","image_alt":"Circuit breaker","slug":"Understanding-Circuit-Breakers-in-Software-Engineering-From-Traditional-to-Serverless","index":"6b86b273ff34f","tags":["System Design","Software Architecture"]}]},"__N_SSG":true}