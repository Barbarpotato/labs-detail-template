{"pageProps":{"article":{"blog_id":"6b4113f2-f30c-4e12-a34a-f5c02abbd1cb","title":"Mastering Apache Spark: An Engaging Dive into Its Architecture and Clusters","short_description":"Welcome to an in-depth exploration of Apache Spark’s architecture! Whether you’re new to Spark or looking to refresh your understanding, this interactive guide will walk you through the key concepts that power Spark’s ability to process massive datasets quickly and efficiently.","description":"<div id=\"content-0\"><h1>Spark Architecture</h1><p>Imagine a Spark application as a bustling city. At the heart of this city is the <strong>Driver Program</strong>, which acts like the mayor overseeing everything that happens. The driver program is responsible for running your code, coordinating work, and making key decisions about how tasks should be executed. Like a city planner, it organizes and manages the tasks to be performed by the cluster, breaking down large jobs into smaller units of work. These jobs are then divided into <strong>tasks</strong> that Spark can run in parallel. The beauty of this architecture lies in its ability to scale, allowing multiple tasks to be completed simultaneously on different data partitions. These tasks are dispatched to <strong>executors</strong>, which are the hard workers of the cluster, doing the heavy lifting.</p></div><div id=\"content-4\"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2FApache-Spark-architecture.png?alt=media&token=ae8b882a-a359-4e63-aad6-a9a9f0a06ff8'/></div><div id=\"content-5\"><p>Executors are independent workers spread across the cluster, each taking responsibility for a portion of the work. They are not just performing tasks but are also responsible for caching data, which can speed up future computations by reusing cached data rather than starting from scratch. Much like how a construction team works efficiently by reusing tools and materials at the job site, executors are optimized to perform computations without redundant effort.</p><p>To help you better visualize this, think of a project that requires breaking down and shipping parts to different locations. Each part can be processed independently and reassembled once all pieces are completed. The driver program ensures these tasks are coordinated correctly, and executors handle each piece of the job.</p><p>But what’s the relationship between <strong>jobs</strong>, <strong>tasks</strong>, and <strong>stages</strong>? Imagine you're the head of a large construction project, and you need to break down the overall project (the <strong>job</strong>) into manageable tasks for each construction team. A <strong>task</strong> in Spark operates on a specific partition of data, much like a construction team focusing on a particular section of the building. Once a group of tasks that don’t depend on any other data is identified, Spark bundles them into <strong>stages</strong>. A stage represents a set of tasks that can be executed independently without needing data from elsewhere in the project. However, sometimes a task will need information from another part of the dataset, requiring what’s known as a <strong>data shuffle</strong>. This shuffle is like coordinating deliveries between different construction teams—an operation that can slow things down due to the necessary data exchange, but essential for the overall completion of the job.</p><p>What happens when Spark needs to run in different environments? That’s where <strong>cluster modes</strong> come in. There are several ways Spark can be deployed, each suited to different use cases. The simplest is <strong>local mode</strong>, ideal for testing on your own machine. Imagine local mode as running your city’s planning department with just one employee—the driver program manages everything on its own, without help from external workers. This is great for testing, but when you need real performance, it’s time to deploy Spark in a cluster.</p><p>In a full cluster setup, Spark supports several modes. <strong>Spark Standalone</strong> is the quickest way to set up a cluster environment, ideal for smaller projects or when you want complete control over the infrastructure. For those already working in large-scale environments with Hadoop, <strong>YARN</strong> is a natural choice. It integrates seamlessly with the broader Hadoop ecosystem, making it easy to manage resources. For greater flexibility and the ability to handle dynamic workloads, <strong>Apache Mesos</strong> comes into play, providing a more robust partitioning and scaling system.</p><p>But if you’re looking for a modern, cloud-native approach, <strong>Kubernetes</strong> offers powerful benefits. Running Spark on Kubernetes is like managing a city that can grow or shrink as needed, using containers to deploy and scale your Spark applications. With Kubernetes, Spark becomes highly portable, making it easy to run your Spark jobs in any cloud environment. You can set up your Spark application inside containers and have them scale automatically based on demand, ensuring smooth processing even as workloads increase.</p></div><div id=\"content-6\"><p><br></p><h1>Mastering the Art of Running Apache Spark Applications</h1><p>Ever wondered how to launch your Apache Spark application into action? Whether you’re processing mountains of data or just running local tests, Apache Spark’s flexibility makes it incredibly powerful. But let’s be real—understanding how to run a Spark application might seem daunting at first. Fear not! Here’s your step-by-step guide to Spark mastery.</p><p>At the heart of it all is the <code>spark-submit</code> script. Think of it as Spark’s personal conductor, ensuring your application runs smoothly across a distributed cluster or right on your local machine. With <code>spark-submit</code>, you’ve got full control: it lets you specify everything from the cluster manager you want to connect to, to how much memory and CPU cores your application needs. You can also include any additional files or libraries your app requires, making sure all the pieces are in place for a flawless run.</p><p>Now, let’s talk dependencies. In Spark, making sure the driver and executors have access to the right libraries is crucial. If you’re using Java or Scala, bundling all your code and libraries into a single uber-JAR (or fat JAR) is a common approach. This neat package ensures that everything is shipped out and accessible where it’s needed. For Python applications—aka PySpark—you’ll want to ensure that each node in your cluster has the exact same Python libraries installed. Imagine trying to run a marathon with mismatched shoes—it won’t end well, right? Same idea with your Spark dependencies.</p><p>If you’re in the mood for a more hands-on, experimental approach, then the <strong>Spark Shell</strong> is your playground. This interactive tool lets you dive right into Spark with either Scala or Python, without needing to write and submit an entire application. When you fire up the Spark Shell, it automatically sets up everything for you—giving you instant access to Spark’s APIs. You can run quick computations, play around with datasets, and see the results in real-time, making it perfect for debugging or just satisfying your curiosity.</p><p>So, to sum it all up: running an Apache Spark application is as easy as using <code>spark-submit</code> to launch your code, bundling your dependencies into an uber-JAR (or ensuring Python libraries are ready), and—if you're feeling adventurous—jumping into the Spark Shell for some interactive magic. Spark truly puts the power of distributed computing at your fingertips.</p></div><div id=\"content-7\"><p>We now trying to submit Apache Spark applications from a python script. This exercise is straightforward thanks to Docker Compose. In this lab, you will:</p><ul><li>Install a Spark Master and Worker using Docker Compose</li><li>Create a python script containing a spark job</li><li>Submit the job to the cluster directly from python (Note: you’ll learn how to submit a job from the command line in the Kubernetes Lab)</li></ul><p><br></p></div><div id=\"content-8\"><h2>Install a Apache Spark cluster using Docker Compose</h2></div><div id=\"content-9\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">git clone https://github.com/big-data-europe/docker-spark</code></pre></div><div id=\"content-11\"><p>change to that directory and attempt to docker-compose up</p></div><div id=\"content-12\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">cd docker-spark\ndocker-compose up</code></pre></div><div id=\"content-13\"><p>After quite some time you should see the following message:</p><p><em>Successfully registered with master spark://&lt;server address&gt;:7077</em></p></div><div id=\"content-14\"><h2>Create Code</h2></div><div id=\"content-15\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">import findspark\nfindspark.init()\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField, StructType, IntegerType, StringType\nsc = SparkContext.getOrCreate(SparkConf().setMaster('spark://localhost:7077'))\nsc.setLogLevel(\"INFO\")\nspark = SparkSession.builder.getOrCreate()\nspark = SparkSession.builder.getOrCreate()\ndf = spark.createDataFrame(\n    [\n        (1, \"foo\"),\n        (2, \"bar\"),\n    ],\n    StructType(\n        [\n            StructField(\"id\", IntegerType(), False),\n            StructField(\"txt\", StringType(), False),\n        ]\n    ),\n)\nprint(df.dtypes)\ndf.show()</code></pre></div><div id=\"content-16\"><h2><strong>Execute code / submit Spark job</strong></h2><p>Now we execute the python file we saved earlier.</p><p>In the terminal, run the following commands to upgrade the pip installer to ensure you have the latest version by running the following commands.Now we execute the python file we saved earlier.</p></div><div id=\"content-17\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">rm -r ~/.cache/pip/selfcheck/\npip3 install --upgrade pip\npip install --upgrade distro-info</code></pre></div><div id=\"content-18\"><p>Please enter the following commands in the terminal to download the spark environment.</p></div><div id=\"content-20\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">wget https://archive.apache.org/dist/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz \n&& tar xf spark-3.3.3-bin-hadoop3.tgz && rm -rf spark-3.3.3-bin-hadoop3.tgz</code></pre></div><div id=\"content-21\"><p>Run the following commands to set up the&nbsp;&nbsp;which is preinstalled in the environment and&nbsp;&nbsp;which you just downloaded.</p></div><div id=\"content-22\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">export JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64\nexport SPARK_HOME=/home/project/spark-3.3.3-bin-hadoop3</code></pre></div><div id=\"content-23\"><p>Install the required packages to set up the spark environment.</p></div><div id=\"content-24\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">pip install pyspark\npython3 -m pip install findspark</code></pre></div><div id=\"content-25\"><p>Type in the following command in the terminal to execute the Python script.</p></div><div id=\"content-26\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">python3 submit.py</code></pre></div><div id=\"content-27\"><p>go to port 8080 to see the admin UI of the Spark master</p></div><div id=\"content-28\"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fdemo-spark-submit.png?alt=media&token=b8fe9a3e-6bcf-42f4-82a7-2b1842b11e74'/></div><div id=\"content-29\"><p><strong>Look at that!</strong> You can now see all your registered workers (we’ve got one for now) and the jobs you’ve submitted (just one at the moment) through Spark's slick interface. Want to dig even deeper? You can access the worker’s UI by heading to port 8081 and see what’s happening under the hood!</p><p>In this hands-on lab, you've set up your very own experimental Apache Spark cluster using Docker Compose. How cool is that? Now, you can easily submit Spark jobs directly from your Python code like a pro!&nbsp;</p><p>But wait, there’s more! In our next adventure—the Kubernetes lab—you’ll unlock the power of submitting Spark jobs right from the command line.</p></div>","timestamp":"Monday, October 7, 2024 at 5:37:09 AM GMT+8","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fspark.png?alt=media&token=7a343f5c-0174-4a31-99e1-4943f9e135af","image_alt":"Apache Spark","slug":"mastering-apache-spark-an-engaging-dive-into-its-architecture-and-clusters","index":"6b86b273ff34f","tags":["Data","Software Architecture","System Design","Storage"]},"recommendedPosts":[{"blog_id":"36855ea7-b37b-4b4c-91f1-27d90b9bde59","title":"Understanding Database Partitioning vs Sharding: Concepts, Benefits, and Challenges","short_description":"When dealing with large volumes of data, efficient database management becomes essential. Two widely used techniques to improve performance and scalability are database partitioning and database sharding. Although often confused, these approaches differ fundamentally in architecture, complexity, and suitable use cases. This article explores these differences in detail, helping you decide which fits your application best.","timestamp":"2025-05-17 09:42:15","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1747474398774_partition%20vs%20sharding%20db.png","image_alt":"partition vs sharding Database","slug":"Understanding-Database-Partitioning-vs-Sharding-Concepts-Benefits-and-Challenges","index":"d4735e3a265e1","tags":["Database","Database Architecture","Software Architecture","System Design"]},{"blog_id":"675f800c-08cb-459f-aa7d-44cdc9c9c169","title":"System Design Simplified: The Trade-Off Triangle You Must Master","short_description":"Behind every well-architected system is a set of tough decisions. The CAP Theorem simplifies those decisions by showing you what you must give up to keep your system fast, correct, and resilient. Learn how to apply this in real-world architecture.","timestamp":"2025-05-13 01:58:48","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1747100859417_CAP%20BG.jpg","image_alt":"CAP background","slug":"System-Design-Simplified-The-Trade-Off-Triangle-You-Must-Master","index":"d4735e3a265e1","tags":["System Design","CAP Theorem","Distributed Systems"]},{"blog_id":"5c2626c4-8a10-47f1-8c5c-b3ac2d84b69a","title":"Why Domain-Driven Design (DDD) Matters: From Chaos to Clarity in Complex Systems","short_description":"Domain-Driven Design (DDD) is a powerful approach to software development that places the business domain—not the technology—at the center of your design decisions. First introduced by Eric Evans, DDD is essential for developers and architects who want to build systems that reflect real-world complexity and change.","timestamp":"2025-05-12 04:23:04","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1747022797381_Domain-Driven-Design_cover.jpg","image_alt":"DDD Cover","slug":"Why-Domain-Driven-Design-DDD-Matters-From-Chaos-to-Clarity-in-Complex-Systems","index":"d4735e3a265e1","tags":["DDD","Software Architecture","System Design"]}]},"__N_SSG":true}