{"pageProps":{"article":{"blog_id":"8be4d7fd-b2ea-4b0f-ad3b-a6064553e06b","title":"Build Kafka Python Client","short_description":"In this lab we will bring back the apache kafka to work with the application environment. In this case we are going to use the apache kafka to a simple python application.","description":"<div id=\"content-0\"><h1><strong>Introduction</strong></h1><p>Previously we already install the apache kafka using the docker compose, create a simple workflow of how the apache is running. You can visit this site to learn the apache kafka installation and the basic workflow if you are not familir with these topic (<a href=\"https://barbarpotato.github.io/#/lab/d1c5319b-3019-4576-9368-d3757bf35c6a\" target=\"_blank\" style=\"color: inherit; background-color: transparent;\">Introduction to Apache Kafka</a>). To create a Kafka Python client that interacts with your Kafka instance, you can use the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">confluent-kafka</code>&nbsp;library, which is a high-performance Kafka client built on the librdkafka C library.</p><h3><strong>Install the Kafka Python Client</strong></h3><p>First, you need to install the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">confluent-kafka</code>&nbsp;library. You can do this using pip. Note: You can create virtual environment of your project folder if you dont want to install this library to your global library package. by using the command: python -m venv &lt;the name of your virtual environment&gt;. then activating the venv using this command: source &lt;your_venv_name&gt;/Scripts/activate.</p></div><div id=\"content-1\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">pip install confluent-kafka</code></pre></div><div id=\"content-2\"><h3><strong>Produce Message</strong></h3><p>Create a Python script to produce messages to the Kafka topic</p></div><div id=\"content-3\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">from confluent_kafka import Producer\nimport socket\n\ndef delivery_report(err, msg):\n    if err is not None:\n        print(f\"Message delivery failed: {err}\")\n    else:\n        print(f\"Message delivered to {msg.topic()} [{msg.partition()}]\")\n\np = Producer({'bootstrap.servers': 'localhost:9092', 'client.id': socket.gethostname()})\n\ntopic = 'test'\n\nfor i in range(10):\n    p.produce(topic, key=str(i), value=f\"Message {i}\", callback=delivery_report)\n    p.poll(0)\n\np.flush()</code></pre></div><div id=\"content-4\"><p></p></div><div id=\"content-5\"><h3><strong>Consume Messages</strong></h3><p>Create another Python script to consume messages from the Kafka topic.</p></div><div id=\"content-6\"><pre style=\"background-color: black; color: white; padding:10px; border-radius: 5px;\"><code style=\"color: white;\">from confluent_kafka import Consumer, KafkaError\n\nc = Consumer({\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'mygroup',\n    'auto.offset.reset': 'earliest'\n})\n\nc.subscribe(['test'])\n\nwhile True:\n    msg = c.poll(1.0)\n\n    if msg is None:\n        continue\n    if msg.error():\n        if msg.error().code() == KafkaError._PARTITION_EOF:\n            print(f'%% {msg.topic()} [{msg.partition()}] reached end at offset {msg.offset()}')\n        elif msg.error():\n            raise KafkaException(msg.error())\n    else:\n        print(f'Received message: {msg.value().decode(\"utf-8\")}')\n\nc.close()</code></pre></div><div id=\"content-7\"><p>the consumer script initializes a Kafka consumer using the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">Consumer</code>&nbsp;class, subscribes to the specified topic(s) with the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">subscribe</code>&nbsp;method, and retrieves messages from the Kafka topic using the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">poll</code>&nbsp;method. Finally, the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">close</code>&nbsp;method closes the consumer and commits the final offsets. This setup allows you to produce and consume messages using your Python Kafka client. Now you can save the file.</p><p><br></p><p>To run the producer and consumer, open two terminal windows or tabs. In the first terminal, run the producer by executing&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">python producer.py</code>, and in the second terminal, run the consumer by executing&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">python consumer.py</code>. The producer script will send 10 messages to the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">test</code>&nbsp;topic, while the consumer script will receive and print these messages. The producer script initializes a Kafka producer using the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">Producer</code>&nbsp;class and sends messages to a specified topic with the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">produce</code>&nbsp;method. A&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">delivery_report</code>&nbsp;callback function confirms message delivery, and the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">poll</code>&nbsp;method serves delivery reports, which should be called regularly. The&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">flush</code>&nbsp;method waits for all messages in the producer queue to be delivered. On the other hand, the consumer script initializes a Kafka consumer using the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">Consumer</code>&nbsp;class, subscribes to the specified topic(s) with the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">subscribe</code>&nbsp;method, and retrieves messages from the Kafka topic using the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">poll</code>&nbsp;method. Finally, the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">close</code>&nbsp;method closes the consumer and commits the final offsets. This setup allows you to produce and consume messages using your Python Kafka client.</p><p><br></p><p>Here is the example output about what we are doing:</p></div><div id=\"content-8\"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_python_client_1.png?alt=media&token=4c079070-9f99-436f-b81c-8b9d0dbada30'/></div><div id=\"content-9\"><img style='width:720px;' src='https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_python_client_2.png?alt=media&token=7a151ac1-e386-4aef-a99a-f2d72fe611fd'/></div><div id=\"content-10\"><h1><strong>Conclusion</strong></h1><p>In conclusion, we successfully set up Apache Kafka using Docker, enabling a reliable environment for message streaming. Following this, we implemented a Kafka Python client using the&nbsp;<code style=\"color: rgb(248, 248, 242); background-color: rgb(39, 40, 34);\">confluent-kafka</code>&nbsp;library, which provides high-performance Kafka producer and consumer capabilities. The producer script was designed to initialize a Kafka producer, send messages to a specified topic, and confirm message delivery through a callback function, while regularly polling for delivery reports and ensuring all messages are delivered with a flush operation. The consumer script was crafted to initialize a Kafka consumer, subscribe to the desired topic(s), retrieve messages, and close the consumer after committing final offsets. By running the producer and consumer scripts in separate terminal windows, you verified the setup by successfully sending and receiving messages. This end-to-end setup demonstrates a functional Kafka environment and a practical Python client for producing and consuming messages, laying a strong foundation for building more complex message-driven applications.</p></div>","timestamp":"Friday, September 20, 2024 at 11:35:05 AM GMT+8","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fkafka_python.png?alt=media&token=10c2db3d-b180-4bad-af2a-90aee92c47e1","image_alt":"Kafka with Pyhton","slug":"build-kafka-python-client","index":"6b86b273ff34f","tags":["Python","Message Broker","Backend"]},"recommendedPosts":[{"blog_id":"06780cd8-d961-479f-90aa-8ce6ffdcfffa","title":"MySQL Migration with Connection Pooling: A Simple Guide","short_description":"Imagine standing in line at a coffee shop where each customer needs to fill out a membership form before ordering and then tears it up after getting their coffee. Sounds inefficient, right? This is exactly what happens when your application connects to a database without connection pooling.","timestamp":"2025-04-28 13:27:45","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1745838714722_connection%20pool%20Bg.png","image_alt":"Connection Pool Labs Content","slug":"MySQL-Migration-with-Connection-Pooling-A-Simple-Guide","index":"d4735e3a265e1","tags":["Database","System Design","SQL","Backend"]},{"blog_id":"6234fef8-1547-46f7-ae10-33d577a1d168","title":"Understanding RabbitMQ: A Favorite Simple Messaging Service!","short_description":"RabbitMQ is a robust, open-source message broker that facilitates communication between applications by sending and receiving messages. Whether you're building a microservices architecture or a distributed system, RabbitMQ ensures reliable, scalable, and asynchronous messaging. In this blog, weâ€™ll walk through its core components and concepts, from producers to consumers, and dive into some advanced features like round-robin dispatching and virtual hosts.","timestamp":"2025-03-15 19:44:13","image":"https://storage.googleapis.com/personal-blog-darmajr.appspot.com/blog-content/1742090540692_rabbitmq.webp","image_alt":"rabbit mq","slug":"Understanding-RabbitMQ-A-Favorite-Simple-Messaging-Service","index":"6b86b273ff34f","tags":["Message Broker","System Design","Software Architecture"]},{"blog_id":"f24fa0d0-6b50-494c-ab4d-49d1b302359f","title":"Building a Robust Microservices Architecture: From gRPC to Kubernetes","short_description":"In the ever-evolving world of software architecture, building a robust and scalable system is key to meeting the demands of modern applications. Recently, I had the opportunity to explore a powerful combination of technologies, starting with gRPC, translating it into HTTP/1.1, and finally deploying the system to a Kubernetes cluster. In this blog, I will take you through the journey, share the challenges I encountered, and explain why each of these steps is important for modern software systems.","timestamp":"2024-12-25 23:34:04","image":"https://firebasestorage.googleapis.com/v0/b/personal-blog-darmajr.appspot.com/o/blog-content%2Fgrpc%20with%20kubernetes.png?alt=media&token=ae61e7f8-2088-416f-924c-512461e18206","image_alt":"Kubernetes+GRPC Background","slug":"Building-a-Robust-Microservices-Architecture-From-gRPC-to-Kubernetes","index":"6b86b273ff34f","tags":["Backend","Software Architecture","System Design","Node.Js","Docker","Cloud Computing"]}]},"__N_SSG":true}